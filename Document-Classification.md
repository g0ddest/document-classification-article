# Классификация документов: 7 практических подходов для небольших наборов данных.

Классификация документов или текста — это одна из основных задач в обработке естественного языка. У нее есть множество применений, таких как классификация новостей, фильтрация спама, поиск неприемлемых комментариев и т. д.

У больших компаний нет проблемы с со сбором больших наборов данных, поэтому обучение модели классификации текста с нуля вполне осуществимая задача, однако, в настоящей жизни в большинстве задач большие наборы данных, как правило, редкость, и для построения своей модели надо искать другие варианты решения.

В этой статье я расскажу о практических подходах к преобразованиям текста, которые сделают возможным классификацию документа на небольших наборах данных.

## Введение в классификацию документов

Классификация документов начинается с очистки и подготовки корпуса из набора данных. Затем этот корпус кодируется любым типом представления текста, который и попадает на этап моделирования.  

![img_0](img/img_0.png)

В этой статье мы остановимся подробно на шаге «Представление Текста» из этой диаграммы.

## Тестовый набор данных для классификации

Мы будем использовать данные из соревнования на сайте kaggle [Правда, или нет? NLP с твитами о катастрофах](https://www.kaggle.com/c/nlp-getting-started). Задачей будет предсказать какие твиты были о настоящих бедствиях, а какие — нет.

Если вы хотите со мной шаг-за-шагом повторять статью, то вам, возможно, стоит установить библиотеки, которые я использовал для данного анализа.

Давайте взяглнем на наши данные:

```python
import pandas as pd

tweet= pd.read_csv('../input/nlp-getting-started/train.csv')
test=pd.read_csv('../input/nlp-getting-started/test.csv')

tweet.head(3)
```

![img_1](img/img_1.png)

В данных содержится id, ключевое слово, место отправки, сам текст твита и результат, который принимает значения 1 и 0. Для предсказания результата мы будем работать только с текстом твита.

```python
print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))
print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))
```

В обучающем наборе данных меньше 8000 твитов, что, принимая во внимание ограничение их 280 символов, делает этот набор небольшим и сложным для работы.

## Подготовка текстовых данных

Прежде, чем мы приступим в работе с NLP[^nlp] нам надо предварительно обработать и очистить данные. Это не цель нашей статьи, но вы можете почитать подробнее об этом шаге в [этой](https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f) статье.

Если коротко, то мы:

* **Токенизируем** — процесс, в котором предложения переводятся в список токенов слов.
* **Избавляемся от стоп-слов** — выкидываем слова вроде "a", или "the".
* **Лемматизируем** — сводим словоформы каждого слова к его нормальной форме, или корню ("studies", "studing" → "study")

```python
def preprocess_news(df):
    '''Function to preprocess and create corpus'''
    new_corpus=[]

    lem=WordNetLemmatizer()
    for text in df["question_text"]:
        words=[w for w in word_tokenize(text) if (w not in stop)]

        words=[lem.lemmatize(w) for w in words]

        new_corpus.append(words)
    return new_corpus

corpus=preprocess_news(df)
```

Теперь давайте посмотрим что мы можем сделать с этим корпусом, чтобы его можно было передать в любой аглоритм машинного обучения.

[nlp]:  (**N**atural **L**anguage **P**rocessing — обработка естественного языка, прим. переводчика)

## Представление текста

Текст нельзя напрямую подать на вход модели машинного обучения, его надо сначала перевести в цифровой формат. Такой шаг называют представлением текста.

### Countvectorizer

Countvectorizer предлагает простой способ векторизовать и представить текстовые документы. Он токенизирует входные данные и строит словарь известных слов, а затем представяет документ используя этот словарь.

Давайте разберем это на примере:

```python
text = ["She sells seashells in the seashore"]
# create the transform
vectorizer = CountVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
# encode document
vector = vectorizer.transform(text)
# summarize encoded vector
print(vector.shape)
print(type(vector))
print(vector.toarray())
```

![img_2](img/img_2.png)

Как вы можете видеть, Countvectorizer построил словарь из переданного текста и представил слова с помощью разреженной матрицы библиотеки [numpy](https://numpy.org/). Мы можем попробовать преобразовать другой текст используя этот словарь и посмотреть на вывод, чтобы получить лучшее понимание работы алгоритма.

```python
vector=vectorizer.transform(["I sell seashells in the seashore"])
vector.toarray()
```

![img_3](img/img_3.png)

Вы можете заметить, что:

* На позициях 3 и 4 мы получили нули, что означает, что эти слова не представлены в нашем словаре, а на других позициях единицы, что означает, что данные слова были найдены в словаре. Соответствующие слова, которые отсутствуют в словаре "sells" и "she". Теперь когда вы понимаете как работает Coutvectorizer, мы можем его использовать для преобразования нашего корпуса.

```python
vec=CountVectorizer(max_df=10,max_features=10000)
vec.fit(df.question_text.values)
vector=vec.transform(df.question_text.values)
```

Вы также должны должны помнить, что у Countvectorizer есть несколько важных параметров, которые вам стоит изменять в зависимости от вашей задачи:

* **max_features** — построить словарь, который будет содержать, только n самых часто встречающихся токенов, отсортированных по популярности в корпусе.
* **min_df** — при построении словаря игнорировать слова, частота токенов которых строго ниже заданного порогового значения.
* **max_df** —  при построении словаря игнорировать слова, частота токенов которых строго выше заданного порогового значения.

Подробный анализ ваших данных обычно помогает в выборе подходящих параметров (или диапазонов для методов оптимизации гиперпараметров).

### TfidfVectorizer

Одна из проблем при работе с Countvectorizer заключается в том, что общие слова, такие как "the" будут повторяться много раз (пока вы не удалите их на этапе предобработки) и эти слова на самом деле не очень важны. Популярной альтернативой является Tfidfvectorizer. Его название — акроним от **Term frequency-inverse document frequency** (частота слова — обратная частота документа).

* **Частота слова (Term Frequency)** — подсчитывает как часто выбранное слово появляется в документе. 
* **Обратная частота документа (Inverse Document Frequency)** — убирает слова, которые часто встречаются в документах.

Давайте взглянем на пример:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
# list of text documents
text = ["She sells seashells by the seashore","The sea.","The seashore"]
# create the transform
vectorizer = TfidfVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
print(vectorizer.idf_)
# encode document
vector = vectorizer.transform([text[0]])
# summarize encoded vector
print(vector.shape)
print(vector.toarray())
```

![img_4](img/img_4.png)

В словаре снова 6 слов и обратная частота слова рассчитана для каждого из них, устанавливая самое низкое значение слову "the", которое встретилось 4 раза.

Затем значения нормализуются до значения между 0 и 1 и данное текстовое представление может быть подано в любую модель машинного обучения.

### Word2vec

Большая проблема в подходах, описанных выше в том, что в предобразовании теряется контекст слова. Вложения слов (embeddings) дает лучшее представление слов в NLP путем кодирования некоторого контекста. Он обеспечивает **отображение слова в соответствующий n-мерный вектор**.

![img_5](/Users/vitaliyvelikodniy/Documents/article/img/img_5.png)

Word2Vec был разработан Томасом Миколовым из Google и использует **неглубокую нейронную сеть** для построения вложений слов. Векторы получаются из контекста, в котором фигурирует слово. В частности, он работает с совместно встречающимися в тексте словами.

Ниже приведена матрица совместного вхождения для предложения "The cat sat on the mat".

![img_6](img/img_6.png)

Word2vec состоит из двух различных моделей:

* **Непрерывный мешок слов** (Continuous Bag of Words, CBOW) можно рассматривать как модель, получающая вложения слов обучая модель **предсказывать слова с учетом их контекста**.
* **Skip-Gram** же наоборот, получает эмбеддинги слов обучая модель **предсказывать контекст по слову.**

Основной идеей вложения слов является то, что слова в схожем контексте, как правило, находятся ближе в вектоном пространстве. Давайте реализуем word2vec на языке python:

```python
import gensim
from gensim.models import Word2Vec

model = gensim.models.Word2Vec(corpus, 
                               min_count = 1, size = 100, window = 5)
```

Когда вы создали свою модель word2vec, можете посмотреть что произойдет, если вы будете изменять важные параметры:

* **size** — определяет размеры вложения результирующего вектора для каждого слова
* **min_count** — минимальное пороговое значение частоты слов в документе при построении словаря.
* **window** — количество слов, окружающих рассматриваемое для построения представления слово. Также известно как размер окна.

В этой статье мы сфокусируемся на практических подходах для небольших наоборов данных и будет использовать предварительно обученные векторы слов вместо обучения векторов из нашего корпуса. Данный метод гарантирует лучшую производительность.

Сначала вам надо скачать обученные векторы [отсюда](https://github.com/mmihaltz/word2vec-GoogleNews-vectors). Затем вы можете загрузить векторы, используя genism.

```python
from  gensim.models.KeyedVectors import load_word2vec_format

def load_word2vec():
    word2vecDict = load_word2vec_format(
        '../input/word2vec-google/GoogleNews-vectors-negative300.bin',
        binary=True, unicode_errors='ignore')
    embeddings_index = dict()
    for word in word2vecDict.wv.vocab:
        embeddings_index[word] = word2vecDict.word_vec(word)

    return embeddings_index
```

Давайте посмотрим на вложение:

```python
w2v_model=load_word2vec()
w2v_model['London'].shape
```

![img_7](img/img_7.png)

Как вы видите слово оказалось представлено 300-мерным вектором. Таким образом, каждое слово из вашего корпуса может быть представлено в таком виде, и эта матрица вложений используется для обучения вашей модели.

### FastText

Теперь давайте рассмотрим чрезвычайно полезный модуль из genism — FastText. Он был разработан Facebook и обеспечивает высокую производительность и скорость в задачах классификации текста.

Он поддерживает модели Continuous Bag of Words и Skip-Gram. Главное различие между предыдщими моделями и FastText в том, что он **разбивает слова на ряд n-грамм**.

Возьмем, к примеру, слово "orange".

Триграммами этого слова будут "ora", "ran", "ang", "nge" (игнорируя начальную и конечную позиции слова).

**Вектор вложения слова (текстовое представление) для "orange" будет суммой этих n-грамм**. Редко встречающиеся слова, или опечатки теперь могут быть правильно представлены, так как высока вероятность, что некоторые из их n-грамм также появляются в других слова.

К примеру, для слова "stupedofantabulouslyfantastic", который никогда не появлялся ни в каком корпусе genism может вернуть или нулевой вектор, или случайный вектор с небольшой величиной.

FastText, тем не менее, **может дать лучший результат разбивая слова на куски и используя вектора этих частей создать результирующий вектор для слова.** В данном примере конечный вектор может быть ближе к векторам "fantastic" и "fantabulous".

Опять же мы будет использовать предварительно обученную модель вместо того, чтобы самостоятельно получать вложения слова.

Для этого сначала скачайте предварительно обученные вектора [отсюда](https://fasttext.cc/docs/en/english-vectors.html).

Каждая линия этого файла содержит слово и соответвующий ему n-мерный вектор. Используя этот файл мы создадим словарь для перевода каждого слова в его векторное представление.

```python
from gensim.models import FastText 

def load_fasttext():
    
	print('loading word embeddings...')
	embeddings_index = {}
	f = open('../input/fasttext/wiki.simple.vec',encoding='utf-8')
	for line in tqdm(f):
    	values = line.strip().rsplit(' ')
    	word = values[0]
    	coefs = np.asarray(values[1:], dtype='float32')
    	embeddings_index[word] = coefs
	f.close()
	print('found %s word vectors' % len(embeddings_index))
    
	return embeddings_index

embeddings_index=load_fastext()
```

![img_8](img/img_8.png)

А теперь посмотрим на вложение:

```python
embeddings_index['london'].shape
```

![img_9](img/img_9.png)

### GloVe

GloVe означает "глобальных вектора для представления слов". Это алгоритм обучения без учителя, разработанный Стэнфордом. Основная его идея состоит в том, чтобы получить семантические отношения между словами используя матрицу совместного использования. Идея очень похожа на word2vec, но есть небольшие [отличия](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010).

Мы будем использовать предварительно обученные на больших корпусах вектора. Это гарантированно лучше работает в практически любой ситуации. Мы можете скачать их [отсюда](https://nlp.stanford.edu/projects/glove/).

После скачивания мы может загрузить нашу предварительно обученную модель. Перед этим вы должны узнать в каком формате она распространяется. В данном случае каждая строка содержит слово и соответствующий ему n-мерный вектор. Как тут:

![img_10](img/img_10.png)

Поэтому сперва вы должны подготовить словарь, который определяет связь между словом и соответствующим вектором. Его можно назвать словарем вложений.

Давайте создадим для наших целей один такой:

```python
def load_glove():
    embedding_dict = {}
    path = '../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'
    with open(path, 'r') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vectors = np.asarray(values[1:], 'float32')
            embedding_dict[word] = vectors
    f.close()

    return embedding_dict


embeddings_index = load_glove()
```

Итак, у нас есть словарь, в котором хранится каждое слово и вектор из предварительно обученного словаря GloVe. Давайте проверим вложение для какого-то слова.

```python
embeddings_index['london'].shape
```

### Универсальное кодирование предложений

![img_11](img/img_11.png)

До сих пор мы рассматривали инструменты, которые наиболее полезны для работы только с представлениями уровня слов. Но иногда нам надо работать на **уровне предложений**. Тут нам помогают кодировщики предложений. 

От такого кодировщика ожидается, что он будет кодировать предложения так, чтобы векторы схожих предложений были максимально близко в векторном пространстве.

К примеру:

* Сегодня солнечная погода.
* Сегодня дождливая погода.
* Сегодня пасмурная погода.

Эти предложения будут закодированы и представлены так, чтобы они были близко друг к другу в векторном пространстве.

Давайте посмотрим как реализовать универсальный кодировщик предложений и найти схожие предложения с его помощью.

Вы можете скачать соответствующие векторы [тут](https://tfhub.dev/google/universal-sentence-encoder/).

Мы загрузим модуль из репозитория предобученных моделей Tensorflow.

```python
module_url = "../input/universalsentenceencoderlarge4"
# Import the Universal Sentence Encoder's TF Hub module
embed = hub.load(module_url)
```

Затем мы создадим вложения для каждого предложения в нашем списке.

```python
sentence_list=df.question_text.values.tolist()
sentence_emb=embed(sentence_list)['outputs'].numpy()
```

Подробнее о универсальных кодировщиках предложений вы можете почитать в этой [статье](https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/).

### Elmo, BERT и другие

При использовании любго из вышеперечисленных методов вложения мы забывает в каком контексте было это слово использовано. Это **один из главных недостатков таких моделей представления слов**.

Благодаря последним разработки в NLP и моделям вроде BERT стало возможным кодировать многозначные слова кодируются разными векторами в зависимости от контекста, в котором оно применялось. Подробнее можно почитать в этой [статье](http://jalammar.github.io/illustrated-bert/).

## Классификация текста.

В этой главе мы подготовим матрицу вложений, которую передадим на слой вложений Keras для получения текстовых представлений. Вы можете повторить те же шаги, чтобы подготовить корпус для любых методов вложений уровня слов.

Давайте проиндексируем слова и установим максимальную длину предложения, заполняя каждое предложение в нашем корпусе с помощью **Keras Tokenizer** и *pad_sequences*.

```python
MAX_LEN=50
tokenizer_obj=Tokenizer()
tokenizer_obj.fit_on_texts(corpus)
sequences=tokenizer_obj.texts_to_sequences(corpus)

tweet_pad=pad_sequences(sequences,
                        maxlen=MAX_LEN,
                        truncating='post',
                        padding='post')
```

Давайте проверим количество уникальных слов в нашем корпусе.

```python
word_index=tokenizer_obj.word_index
print('Number of unique words:',len(word_index))
```

Использую данный словарь индекса слов и словарь вложений, мы можете создать матрицу вложений для нашего корпуса. Эта матрица вложений передается на соответствующий уровень нейронной сети для получения представлений слов.

```python
def prepare_matrix(embedding_dict, emb_size=300):
    num_words = len(word_index)
    embedding_matrix = np.zeros((num_words, emb_size))

    for word, i in tqdm(word_index.items()):
        if i > num_words:
            continue

    emb_vec = embedding_dict.get(word)
    if emb_vec is not None:
        embedding_matrix[i] = emb_vec

    return embedding_matrix
```

Мы можем определить нашу нейронную сеть и передать индекс вложений на слой вложений сети. Для этого мы передаем векторы на слой вложений и устанавливаем **trainable=False**, чтобы предотвратить изменение весов.

```python
def new_model(embedding_matrix):
    inp = Input(shape=(MAX_LEN,))

    x = Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix],
                  trainable=False)(inp)

    x = Bidirectional(
        LSTM(60, return_sequences=True, name='lstm_layer', 
             dropout=0.1, recurrent_dropout=0.1))(x)

    x = GlobalAveragePool1D()(x)
    x = Dense(1, activation="sigmoid")(x)
    model = Model(inputs=inp, outputs=x)

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    return model
```

К примеру, запустим модель, используя вложения word2vec:

```python
embeddings_index=load_word2vec()
embedding_matrix=prepare_matrix(embeddings_index)
model=new_model(embedding_matrix)

history=model.fit(X_train,y_train,
                  batch_size=8,
                  epochs=5,
                  validation_data=(X_test,y_test),
                  verbose=2)
```

![img_12](img/img_12.png)

Вы можете вызвать нужный вам тип вложений и выполнить те же действия для реализации каждого из них.

## Сравнение

Итак, какой из методов классификации работает лучше всего в нашей задаче?

Можно использовать Neptune для сравнения производительности нашей модели с использованием различных вложений.

![img_13](img/img_13.png)

Вложения glove в тестовых наборах данных было немного более производительным, по сравнению с другими двумя вложениями. Возможно, вы добьетесь лучших результатов почистив данных и настроив дополнительно модель.

Можете посмотреть на проведенные эксперименты [тут](https://ui.neptune.ai/shared/blog-text-classification/experiments?viewId=a68c3b2a-65c0-412d-94ca-c0c749e0a84f).

## Заключение

В этой статье мы рассмотрели и реализовали различные методы представления признаков для классификации текста, которые можно использовать для небольших наборов данных. 

Надеюсь, что вы найдете их полезными для использования в своих проектах.